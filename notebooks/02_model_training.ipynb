{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hibadash/-Breast-Ultrasound-Classification-Using-Xception-CNN-BUSI-Dataset/blob/ModelTraining/notebooks/02_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI0jzVUH06C9"
      },
      "outputs": [],
      "source": [
        "# Notebook 02 ‚Äî Entra√Ænement du mod√®le Xception\n",
        "\n",
        "Ce notebook contient le code complet pour entra√Æner le mod√®le Xception CNN sur le dataset BUSI.\n",
        "\n",
        "## Objectifs:\n",
        "- Charger et pr√©parer les donn√©es\n",
        "- Configurer les hyperparam√®tres (learning rate, batch size, epochs)\n",
        "- Entra√Æner le mod√®le avec callbacks (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau)\n",
        "- Visualiser les r√©sultats d'entra√Ænement\n",
        "- Sauvegarder le meilleur mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGPNxtO0x1oT",
        "outputId": "8c225fe3-1173-4c85-d872-ac41615622aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'breast_project'...\n",
            "remote: Enumerating objects: 1636, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 1636 (delta 24), reused 14 (delta 1), pack-reused 1584 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1636/1636), 194.53 MiB | 19.70 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n",
            "Updating files: 100% (1589/1589), done.\n",
            "/content/breast_project\n",
            "app  data  notebooks  README.md  requirements.txt  results  src\n"
          ]
        }
      ],
      "source": [
        "# Cloner le repo GitHub dans un dossier local \"breast_project\"\n",
        "import os\n",
        "\n",
        "# V√©rifie si le dossier \"breast_project\" existe, sinon clone dedans\n",
        "if not os.path.exists(\"breast_project\"):\n",
        "    !git clone https://github.com/hibadash/-Breast-Ultrasound-Classification-Using-Xception-CNN-BUSI-Dataset.git breast_project\n",
        "\n",
        "# Aller dans le dossier clon√© (utiliser le chemin absolu pour √©viter les imbrications)\n",
        "import os\n",
        "breast_project_path = os.path.abspath(\"breast_project\")\n",
        "os.chdir(breast_project_path)\n",
        "\n",
        "# V√©rifier qu'on est dans le bon r√©pertoire\n",
        "print(f\"‚úÖ R√©pertoire courant: {os.getcwd()}\")\n",
        "print(f\"‚úÖ Contenu du projet:\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5GpC7LO8xm4"
      },
      "outputs": [],
      "source": [
        "# Installer les d√©pendances si n√©cessaire\n",
        "!pip install -q tensorflow>=2.15 numpy pandas matplotlib seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrvc6gDm8xm6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import (\n",
        "    ModelCheckpoint,\n",
        "    EarlyStopping,\n",
        "    ReduceLROnPlateau,\n",
        "    CSVLogger,\n",
        "    TensorBoard\n",
        ")\n",
        "\n",
        "# Afficher les versions\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# V√©rifier la disponibilit√© du GPU\n",
        "print(f\"\\nGPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU d√©tect√©\")\n",
        "else:\n",
        "    print(\"Pas de GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaO8FQu58xm7"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# HYPERPARAM√àTRES D'ENTRA√éNEMENT\n",
        "# =======================\n",
        "\n",
        "# Hyperparam√®tres principaux\n",
        "LEARNING_RATE = 1e-4  # Taux d'apprentissage initial\n",
        "BATCH_SIZE = 32  # Taille du batch\n",
        "EPOCHS = 50  # Nombre maximum d'epochs\n",
        "IMAGE_SIZE = (224, 224)  # Taille des images\n",
        "\n",
        "# Callbacks\n",
        "PATIENCE_EARLY_STOPPING = 10  # Arr√™t anticip√© si pas d'am√©lioration\n",
        "PATIENCE_LR_REDUCTION = 5  # R√©duction du LR si pas d'am√©lioration\n",
        "\n",
        "# Fine-tuning (optionnel)\n",
        "FINE_TUNE_AFTER_EPOCHS = 20  # Commencer le fine-tuning apr√®s N epochs\n",
        "FINE_TUNE_LEARNING_RATE = 1e-5  # LR plus faible pour fine-tuning\n",
        "\n",
        "# Chemins\n",
        "DATASET_DIR = 'data/Dataset_BUSI'\n",
        "RESULTS_DIR = 'results'\n",
        "MODEL_SAVE_PATH = os.path.join(RESULTS_DIR, 'model_xception_best.h5')\n",
        "MODEL_FINAL_PATH = os.path.join(RESULTS_DIR, 'model_xception_final.h5')\n",
        "\n",
        "# Cr√©er le dossier results\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Configuration charg√©e!\")\n",
        "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   - Epochs max: {EPOCHS}\")\n",
        "print(f\"   - Early stopping patience: {PATIENCE_EARLY_STOPPING}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsMA_pwo8xm9"
      },
      "source": [
        "## Pr√©paration des donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uMRddS28xnA"
      },
      "outputs": [],
      "source": [
        "# V√©rifier et corriger preprocess.py AVANT l'import\n",
        "import os\n",
        "\n",
        "# V√©rifier le contenu actuel de preprocess.py\n",
        "preprocess_file = 'data/preprocess.py'\n",
        "if os.path.exists(preprocess_file):\n",
        "    with open(preprocess_file, 'r') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    # V√©rifier si le fichier a le bon code\n",
        "    if \"DATASET_DIR = os.path.join(_project_root, 'data', 'Dataset_BUSI')\" not in content:\n",
        "        print(\"‚ö†Ô∏è  Correction de preprocess.py...\")\n",
        "        # Remplacer l'ancien code par le nouveau\n",
        "        import re\n",
        "        \n",
        "        # Pattern pour trouver l'ancien DATASET_DIR\n",
        "        old_patterns = [\n",
        "            r\"DATASET_DIR\\s*=\\s*['\\\"]Dataset_BUSI['\\\"]\",\n",
        "            r\"DATASET_DIR\\s*=\\s*['\\\"]data/Dataset_BUSI['\\\"]\",\n",
        "        ]\n",
        "        \n",
        "        new_code = \"\"\"# Chemin du dataset - bas√© sur l'emplacement de ce fichier\n",
        "_current_file_dir = os.path.dirname(os.path.abspath(__file__))  \n",
        "_project_root = os.path.dirname(_current_file_dir) \n",
        "DATASET_DIR = os.path.join(_project_root, 'data', 'Dataset_BUSI')\n",
        "\n",
        "# Debug: Afficher le chemin calcul√©\n",
        "print(f\"[DEBUG preprocess.py] __file__ = {__file__}\")\n",
        "print(f\"[DEBUG preprocess.py] DATASET_DIR = {DATASET_DIR}\")\n",
        "print(f\"[DEBUG preprocess.py] DATASET_DIR existe? {os.path.exists(DATASET_DIR)}\")\"\"\"\n",
        "        \n",
        "        # Remplacer\n",
        "        for pattern in old_patterns:\n",
        "            content = re.sub(pattern + r'.*?\\n', '', content, flags=re.MULTILINE)\n",
        "        \n",
        "        # Ins√©rer le nouveau code apr√®s BATCH_SIZE\n",
        "        content = re.sub(\n",
        "            r'(BATCH_SIZE\\s*=\\s*\\d+)',\n",
        "            r'\\1\\n\\n' + new_code,\n",
        "            content\n",
        "        )\n",
        "        \n",
        "        # Sauvegarder\n",
        "        with open(preprocess_file, 'w') as f:\n",
        "            f.write(content)\n",
        "        print(\"‚úÖ preprocess.py corrig√©!\")\n",
        "    else:\n",
        "        print(\"‚úÖ preprocess.py est √† jour\")\n",
        "else:\n",
        "    print(f\"‚ùå Fichier {preprocess_file} non trouv√©!\")\n",
        "\n",
        "# V√©rifier le chemin du dataset\n",
        "print(f\"\\nüìÅ V√©rification du dataset...\")\n",
        "print(f\"   R√©pertoire courant: {os.getcwd()}\")\n",
        "print(f\"   Chemin dataset (notebook): {DATASET_DIR}\")\n",
        "print(f\"   Dataset existe? {os.path.exists(DATASET_DIR)}\")\n",
        "\n",
        "if not os.path.exists(DATASET_DIR):\n",
        "    print(f\"\\n‚ùå ERREUR: Dataset non trouv√© dans {DATASET_DIR}\")\n",
        "    raise FileNotFoundError(f\"Dataset non trouv√©: {DATASET_DIR}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset trouv√©!\")\n",
        "    train_path = os.path.join(DATASET_DIR, 'train')\n",
        "    val_path = os.path.join(DATASET_DIR, 'validation')\n",
        "    test_path = os.path.join(DATASET_DIR, 'test')\n",
        "    print(f\"   - Train existe: {os.path.exists(train_path)}\")\n",
        "    print(f\"   - Validation existe: {os.path.exists(val_path)}\")\n",
        "    print(f\"   - Test existe: {os.path.exists(test_path)}\")\n",
        "    print()\n",
        "\n",
        "# Importer les g√©n√©rateurs de donn√©es depuis preprocess.py\n",
        "from data.preprocess import (\n",
        "    train_generator,\n",
        "    val_generator,\n",
        "    test_generator\n",
        ")\n",
        "\n",
        "# Afficher les informations sur le dataset\n",
        "print(\"üìä INFORMATIONS SUR LE DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Images d'entra√Ænement: {train_generator.samples}\")\n",
        "print(f\"Images de validation: {val_generator.samples}\")\n",
        "print(f\"Images de test: {test_generator.samples}\")\n",
        "print(f\"\\nClasses: {sorted(list(train_generator.class_indices.keys()))}\")\n",
        "print(f\"Mapping des classes: {train_generator.class_indices}\")\n",
        "print(f\"Taille des images: {IMAGE_SIZE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVjqE6Os8xnA"
      },
      "source": [
        "## Construction du mod√®le Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "273gqTrw8xnB"
      },
      "outputs": [],
      "source": [
        "# Importer la fonction de cr√©ation du mod√®le\n",
        "from src.model import load_xception_model\n",
        "\n",
        "# Cr√©er le mod√®le\n",
        "num_classes = len(train_generator.class_indices)\n",
        "print(f\"Cr√©ation du mod√®le Xception pour {num_classes} classes...\")\n",
        "\n",
        "model = load_xception_model(\n",
        "    input_shape=(*IMAGE_SIZE, 3),\n",
        "    num_classes=num_classes,\n",
        "    trainable=False  # Commencer avec les couches de base gel√©es\n",
        ")\n",
        "\n",
        "print(\"Mod√®le cr√©√© avec succ√®s!\")\n",
        "print(f\"\\n Statistiques du mod√®le:\")\n",
        "print(f\"   - Param√®tres totaux: {model.count_params():,}\")\n",
        "print(f\"   - Param√®tres entra√Ænables: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
        "\n",
        "# Afficher un r√©sum√© du mod√®le\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zevcDKG8xnB"
      },
      "source": [
        "## Configuration des callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpv1h0Nn8xnC"
      },
      "outputs": [],
      "source": [
        "# Cr√©er tous les callbacks n√©cessaires\n",
        "callbacks = [\n",
        "    # 1. Sauvegarde du meilleur mod√®le\n",
        "    ModelCheckpoint(\n",
        "        filepath=MODEL_SAVE_PATH,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        verbose=1,\n",
        "        save_fmt='h5'\n",
        "    ),\n",
        "\n",
        "    # 2. Early Stopping\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        patience=PATIENCE_EARLY_STOPPING,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        min_delta=0.0001\n",
        "    ),\n",
        "\n",
        "    # 3. R√©duction automatique du learning rate\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        factor=0.5,  # R√©duire le LR de moiti√©\n",
        "        patience=PATIENCE_LR_REDUCTION,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # 4. Logger CSV\n",
        "    CSVLogger(\n",
        "        filename=os.path.join(RESULTS_DIR, 'training_log.csv'),\n",
        "        separator=',',\n",
        "        append=False\n",
        "    ),\n",
        "\n",
        "    # 5. TensorBoard (pour Colab, on peut utiliser TensorBoard.dev)\n",
        "    TensorBoard(\n",
        "        log_dir=os.path.join(RESULTS_DIR, 'tensorboard_logs'),\n",
        "        histogram_freq=1,\n",
        "        write_graph=True,\n",
        "        update_freq='epoch'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\" {len(callbacks)} callbacks configur√©s:\")\n",
        "print(\"   1. ModelCheckpoint - Sauvegarde du meilleur mod√®le\")\n",
        "print(\"   2. EarlyStopping - Arr√™t anticip√©\")\n",
        "print(\"   3. ReduceLROnPlateau - R√©duction du learning rate\")\n",
        "print(\"   4. CSVLogger - Log des m√©triques\")\n",
        "print(\"   5. TensorBoard - Visualisation avanc√©e\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tZIuIoG8xnC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X98NyUR48xnC"
      },
      "outputs": [],
      "source": [
        "# Calculer les steps par epoch\n",
        "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
        "validation_steps = val_generator.samples // BATCH_SIZE\n",
        "\n",
        "print(\"D√âBUT DE L'ENTRA√éNEMENT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Steps par epoch: {steps_per_epoch}\")\n",
        "print(f\"Validation steps: {validation_steps}\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# Lancer l'entra√Ænement\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n ENTRA√éNEMENT TERMIN√â!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qArWqDW_8xnD"
      },
      "source": [
        "## Sauvegarde du mod√®le final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSS6-6mQ8xnD"
      },
      "outputs": [],
      "source": [
        "# Sauvegarder le mod√®le final (apr√®s tous les epochs)\n",
        "model.save(MODEL_FINAL_PATH)\n",
        "print(f\" Mod√®le final sauvegard√©: {MODEL_FINAL_PATH}\")\n",
        "\n",
        "# Sauvegarder l'historique en JSON\n",
        "history_dict = {}\n",
        "for key, values in history.history.items():\n",
        "    history_dict[key] = [float(v) for v in values]\n",
        "\n",
        "history_path = os.path.join(RESULTS_DIR, 'training_history.json')\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(history_dict, f, indent=2)\n",
        "\n",
        "print(f\" Historique sauvegard√©: {history_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSH3u84m8xnD"
      },
      "source": [
        "## Visualisation des r√©sultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ispz2Z2l8xnE"
      },
      "outputs": [],
      "source": [
        "# Cr√©er les graphiques d'entra√Ænement\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Loss\n",
        "axes[0].plot(history.history['loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss', marker='s', linewidth=2)\n",
        "axes[0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Accuracy\n",
        "axes[1].plot(history.history['accuracy'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s', linewidth=2)\n",
        "axes[1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(RESULTS_DIR, 'training_history_plot.png')\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "print(f\" Graphique sauvegard√©: {plot_path}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c60HeH3j8xnE"
      },
      "source": [
        "## R√©sum√© des performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfhI2KhD8xnE"
      },
      "outputs": [],
      "source": [
        "# Afficher un r√©sum√© des performances\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"R√âSUM√â DE L'ENTRA√éNEMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
        "\n",
        "print(f\"   - Epochs effectu√©s: {len(history.history['loss'])}\")\n",
        "print(f\"   - Meilleure validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%) - Epoch {best_epoch}\")\n",
        "print(f\"   - Accuracy finale (train): {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
        "print(f\"   - Accuracy finale (validation): {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
        "print(f\"   - Loss finale (train): {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"   - Loss finale (validation): {history.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "# Afficher le learning rate final si disponible\n",
        "if 'lr' in history.history:\n",
        "    final_lr = history.history['lr'][-1]\n",
        "    print(f\"   - Learning rate final: {final_lr:.2e}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n Mod√®le sauvegard√© dans: {MODEL_SAVE_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
