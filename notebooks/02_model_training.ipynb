{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hibadash/-Breast-Ultrasound-Classification-Using-Xception-CNN-BUSI-Dataset/blob/ModelTraining/notebooks/02_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI0jzVUH06C9"
      },
      "outputs": [],
      "source": [
        "# Notebook 02 — Entraînement du modèle Xception\n",
        "\n",
        "Ce notebook contient le code complet pour entraîner le modèle Xception CNN sur le dataset BUSI.\n",
        "\n",
        "## Objectifs:\n",
        "- Charger et préparer les données\n",
        "- Configurer les hyperparamètres (learning rate, batch size, epochs)\n",
        "- Entraîner le modèle avec callbacks (EarlyStopping, ModelCheckpoint, ReduceLROnPlateau)\n",
        "- Visualiser les résultats d'entraînement\n",
        "- Sauvegarder le meilleur modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGPNxtO0x1oT",
        "outputId": "8c225fe3-1173-4c85-d872-ac41615622aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'breast_project'...\n",
            "remote: Enumerating objects: 1636, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 1636 (delta 24), reused 14 (delta 1), pack-reused 1584 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1636/1636), 194.53 MiB | 19.70 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n",
            "Updating files: 100% (1589/1589), done.\n",
            "/content/breast_project\n",
            "app  data  notebooks  README.md  requirements.txt  results  src\n"
          ]
        }
      ],
      "source": [
        "# Cloner le repo GitHub dans un dossier local \"breast_project\"\n",
        "import os\n",
        "\n",
        "# Vérifie si le dossier \"breast_project\" existe, sinon clone dedans\n",
        "if not os.path.exists(\"breast_project\"):\n",
        "    !git clone https://github.com/hibadash/-Breast-Ultrasound-Classification-Using-Xception-CNN-BUSI-Dataset.git breast_project\n",
        "\n",
        "# Aller dans le dossier cloné (utiliser le chemin absolu pour éviter les imbrications)\n",
        "import os\n",
        "breast_project_path = os.path.abspath(\"breast_project\")\n",
        "os.chdir(breast_project_path)\n",
        "\n",
        "# Vérifier qu'on est dans le bon répertoire\n",
        "print(f\" Répertoire courant: {os.getcwd()}\")\n",
        "print(f\" Contenu du projet:\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5GpC7LO8xm4"
      },
      "outputs": [],
      "source": [
        "# Installer les dépendances si nécessaire\n",
        "!pip install -q tensorflow>=2.15 numpy pandas matplotlib seaborn scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrvc6gDm8xm6"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import (\n",
        "    ModelCheckpoint,\n",
        "    EarlyStopping,\n",
        "    ReduceLROnPlateau,\n",
        "    CSVLogger,\n",
        "    TensorBoard\n",
        ")\n",
        "\n",
        "# Afficher les versions\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Vérifier la disponibilité du GPU\n",
        "print(f\"\\nGPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU détecté\")\n",
        "else:\n",
        "    print(\"Pas de GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaO8FQu58xm7"
      },
      "outputs": [],
      "source": [
        "# =======================\n",
        "# HYPERPARAMÈTRES D'ENTRAÎNEMENT\n",
        "# =======================\n",
        "\n",
        "# Hyperparamètres principaux\n",
        "LEARNING_RATE = 1e-4  # Taux d'apprentissage initial\n",
        "BATCH_SIZE = 32  # Taille du batch\n",
        "EPOCHS = 50  # Nombre maximum d'epochs\n",
        "IMAGE_SIZE = (224, 224)  # Taille des images\n",
        "\n",
        "# Callbacks\n",
        "PATIENCE_EARLY_STOPPING = 10  # Arrêt anticipé si pas d'amélioration\n",
        "PATIENCE_LR_REDUCTION = 5  # Réduction du LR si pas d'amélioration\n",
        "\n",
        "# Fine-tuning (optionnel)\n",
        "FINE_TUNE_AFTER_EPOCHS = 20  # Commencer le fine-tuning après N epochs\n",
        "FINE_TUNE_LEARNING_RATE = 1e-5  # LR plus faible pour fine-tuning\n",
        "\n",
        "# Chemins\n",
        "DATASET_DIR = 'data/Dataset_BUSI'\n",
        "RESULTS_DIR = 'results'\n",
        "MODEL_SAVE_PATH = os.path.join(RESULTS_DIR, 'model_xception_best.h5')\n",
        "MODEL_FINAL_PATH = os.path.join(RESULTS_DIR, 'model_xception_final.h5')\n",
        "\n",
        "# Créer le dossier results\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Configuration chargée!\")\n",
        "print(f\"   - Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   - Epochs max: {EPOCHS}\")\n",
        "print(f\"   - Early stopping patience: {PATIENCE_EARLY_STOPPING}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsMA_pwo8xm9"
      },
      "source": [
        "## Préparation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uMRddS28xnA"
      },
      "outputs": [],
      "source": [
        "# Vérifier le chemin du dataset AVANT d'importer preprocess.py\n",
        "import os\n",
        "print(f\"   Vérification du dataset...\")\n",
        "print(f\"   Répertoire courant: {os.getcwd()}\")\n",
        "print(f\"   Chemin dataset: {DATASET_DIR}\")\n",
        "print(f\"   Dataset existe? {os.path.exists(DATASET_DIR)}\")\n",
        "\n",
        "# Importer les générateurs de données depuis preprocess.py\n",
        "from data.preprocess import (\n",
        "    train_generator,\n",
        "    val_generator,\n",
        "    test_generator\n",
        ")\n",
        "\n",
        "# Afficher les informations sur le dataset\n",
        "print(\" INFORMATIONS SUR LE DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Images d'entraînement: {train_generator.samples}\")\n",
        "print(f\"Images de validation: {val_generator.samples}\")\n",
        "print(f\"Images de test: {test_generator.samples}\")\n",
        "print(f\"\\nClasses: {sorted(list(train_generator.class_indices.keys()))}\")\n",
        "print(f\"Mapping des classes: {train_generator.class_indices}\")\n",
        "print(f\"Taille des images: {IMAGE_SIZE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVjqE6Os8xnA"
      },
      "source": [
        "## Construction du modèle Xception"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "273gqTrw8xnB"
      },
      "outputs": [],
      "source": [
        "# Importer la fonction de création du modèle\n",
        "from src.model import load_xception_model\n",
        "\n",
        "# Créer le modèle\n",
        "num_classes = len(train_generator.class_indices)\n",
        "print(f\"Création du modèle Xception pour {num_classes} classes...\")\n",
        "\n",
        "model = load_xception_model(\n",
        "    input_shape=(*IMAGE_SIZE, 3),\n",
        "    num_classes=num_classes,\n",
        "    trainable=False  # Commencer avec les couches de base gelées\n",
        ")\n",
        "\n",
        "print(\"Modèle créé avec succès!\")\n",
        "print(f\"\\n Statistiques du modèle:\")\n",
        "print(f\"   - Paramètres totaux: {model.count_params():,}\")\n",
        "print(f\"   - Paramètres entraînables: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}\")\n",
        "\n",
        "# Afficher un résumé du modèle\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zevcDKG8xnB"
      },
      "source": [
        "## Configuration des callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpv1h0Nn8xnC"
      },
      "outputs": [],
      "source": [
        "# Créer tous les callbacks nécessaires\n",
        "callbacks = [\n",
        "    # 1. Sauvegarde du meilleur modèle\n",
        "    ModelCheckpoint(\n",
        "        filepath=MODEL_SAVE_PATH,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        verbose=1,\n",
        "        save_fmt='h5'\n",
        "    ),\n",
        "\n",
        "    # 2. Early Stopping\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        patience=PATIENCE_EARLY_STOPPING,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1,\n",
        "        min_delta=0.0001\n",
        "    ),\n",
        "\n",
        "    # 3. Réduction automatique du learning rate\n",
        "    ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        factor=0.5,  # Réduire le LR de moitié\n",
        "        patience=PATIENCE_LR_REDUCTION,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    ),\n",
        "\n",
        "    # 4. Logger CSV\n",
        "    CSVLogger(\n",
        "        filename=os.path.join(RESULTS_DIR, 'training_log.csv'),\n",
        "        separator=',',\n",
        "        append=False\n",
        "    ),\n",
        "\n",
        "    # 5. TensorBoard (pour Colab, on peut utiliser TensorBoard.dev)\n",
        "    TensorBoard(\n",
        "        log_dir=os.path.join(RESULTS_DIR, 'tensorboard_logs'),\n",
        "        histogram_freq=1,\n",
        "        write_graph=True,\n",
        "        update_freq='epoch'\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\" {len(callbacks)} callbacks configurés:\")\n",
        "print(\"   1. ModelCheckpoint - Sauvegarde du meilleur modèle\")\n",
        "print(\"   2. EarlyStopping - Arrêt anticipé\")\n",
        "print(\"   3. ReduceLROnPlateau - Réduction du learning rate\")\n",
        "print(\"   4. CSVLogger - Log des métriques\")\n",
        "print(\"   5. TensorBoard - Visualisation avancée\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tZIuIoG8xnC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X98NyUR48xnC"
      },
      "outputs": [],
      "source": [
        "# Calculer les steps par epoch\n",
        "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
        "validation_steps = val_generator.samples // BATCH_SIZE\n",
        "\n",
        "print(\"DÉBUT DE L'ENTRAÎNEMENT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Steps par epoch: {steps_per_epoch}\")\n",
        "print(f\"Validation steps: {validation_steps}\")\n",
        "print(\"=\"*60)\n",
        "print()\n",
        "\n",
        "# Lancer l'entraînement\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n ENTRAÎNEMENT TERMINÉ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qArWqDW_8xnD"
      },
      "source": [
        "## Sauvegarde du modèle final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSS6-6mQ8xnD"
      },
      "outputs": [],
      "source": [
        "# Sauvegarder le modèle final (après tous les epochs)\n",
        "model.save(MODEL_FINAL_PATH)\n",
        "print(f\" Modèle final sauvegardé: {MODEL_FINAL_PATH}\")\n",
        "\n",
        "# Sauvegarder l'historique en JSON\n",
        "history_dict = {}\n",
        "for key, values in history.history.items():\n",
        "    history_dict[key] = [float(v) for v in values]\n",
        "\n",
        "history_path = os.path.join(RESULTS_DIR, 'training_history.json')\n",
        "with open(history_path, 'w') as f:\n",
        "    json.dump(history_dict, f, indent=2)\n",
        "\n",
        "print(f\" Historique sauvegardé: {history_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSH3u84m8xnD"
      },
      "source": [
        "## Visualisation des résultats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ispz2Z2l8xnE"
      },
      "outputs": [],
      "source": [
        "# Créer les graphiques d'entraînement\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Loss\n",
        "axes[0].plot(history.history['loss'], label='Train Loss', marker='o', linewidth=2)\n",
        "axes[0].plot(history.history['val_loss'], label='Validation Loss', marker='s', linewidth=2)\n",
        "axes[0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Accuracy\n",
        "axes[1].plot(history.history['accuracy'], label='Train Accuracy', marker='o', linewidth=2)\n",
        "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s', linewidth=2)\n",
        "axes[1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(RESULTS_DIR, 'training_history_plot.png')\n",
        "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "print(f\" Graphique sauvegardé: {plot_path}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c60HeH3j8xnE"
      },
      "source": [
        "## Résumé des performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfhI2KhD8xnE"
      },
      "outputs": [],
      "source": [
        "# Afficher un résumé des performances\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RÉSUMÉ DE L'ENTRAÎNEMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
        "\n",
        "print(f\"   - Epochs effectués: {len(history.history['loss'])}\")\n",
        "print(f\"   - Meilleure validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%) - Epoch {best_epoch}\")\n",
        "print(f\"   - Accuracy finale (train): {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
        "print(f\"   - Accuracy finale (validation): {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
        "print(f\"   - Loss finale (train): {history.history['loss'][-1]:.4f}\")\n",
        "print(f\"   - Loss finale (validation): {history.history['val_loss'][-1]:.4f}\")\n",
        "\n",
        "# Afficher le learning rate final si disponible\n",
        "if 'lr' in history.history:\n",
        "    final_lr = history.history['lr'][-1]\n",
        "    print(f\"   - Learning rate final: {final_lr:.2e}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n Modèle sauvegardé dans: {MODEL_SAVE_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
